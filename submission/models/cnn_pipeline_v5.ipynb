{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CNN Pipeline v3","metadata":{"id":"JRtl6hB-vs_U"}},{"cell_type":"markdown","source":"## Environment settings","metadata":{"id":"hsXz12ot7PBB"}},{"cell_type":"markdown","source":"### Libraries","metadata":{"id":"-PoKdpXXtab1"}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom PIL import Image, ImageFilter, ImageEnhance\nfrom numpy import asarray\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom IPython.display import clear_output\nimport math\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nprint(tf.__version__)","metadata":{"id":"fVT6JS9FvzwC","outputId":"6bdf52bd-fa25-4072-c484-2000b693b4c6","execution":{"iopub.status.busy":"2021-11-17T20:33:02.730823Z","iopub.execute_input":"2021-11-17T20:33:02.731212Z","iopub.status.idle":"2021-11-17T20:33:09.457643Z","shell.execute_reply.started":"2021-11-17T20:33:02.731106Z","shell.execute_reply":"2021-11-17T20:33:09.456530Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Random seed","metadata":{"id":"T4Q-CAiNtnRP"}},{"cell_type":"code","source":"# Random seed for reproducibility\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{"id":"lJfQgW-bto8I","execution":{"iopub.status.busy":"2021-11-17T20:33:14.809148Z","iopub.execute_input":"2021-11-17T20:33:14.809443Z","iopub.status.idle":"2021-11-17T20:33:14.815185Z","shell.execute_reply.started":"2021-11-17T20:33:14.809413Z","shell.execute_reply":"2021-11-17T20:33:14.814265Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Data Loader","metadata":{"id":"Ix1cVRCF7Svm"}},{"cell_type":"code","source":"# google drive plug-in\nfrom google.colab import drive\ndrive.mount('/gdrive')","metadata":{"id":"Znsy7xWLwVJN","outputId":"f80ca3a2-eb7c-426a-fb4e-b4e9839ed9c1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# directory\n%cd /gdrive/MyDrive/notebooks/an2dl_homeworks/training_validation_testing/mirko","metadata":{"id":"v4NxuSzOwXCT","outputId":"1a36ae72-0e36-4611-a1c7-6a0a0347d12a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# paths\nroot_path = '/kaggle/input/leaves/leaves/'#'../../training_validation_testing/mirko/'\ntraining_dir = os.path.join(root_path, 'training')\nvalidation_dir = os.path.join(root_path, 'validation')\ntesting_dir = os.path.join(root_path, 'testing')","metadata":{"id":"fN5i13swxHia","execution":{"iopub.status.busy":"2021-11-17T20:34:27.049960Z","iopub.execute_input":"2021-11-17T20:34:27.050299Z","iopub.status.idle":"2021-11-17T20:34:27.056304Z","shell.execute_reply.started":"2021-11-17T20:34:27.050228Z","shell.execute_reply":"2021-11-17T20:34:27.055168Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Data Pre-Processing","metadata":{"id":"Rvwrd5Zz8rmC"}},{"cell_type":"markdown","source":"### Model statistics","metadata":{"id":"5EsLw3pTtDYg"}},{"cell_type":"markdown","source":"#### Set pre-computed statistics","metadata":{"id":"DbggF9aRubWs"}},{"cell_type":"code","source":"# MEAN CENTER SHIFT\nz_score_mean = 0\n\n# STD NORMALIZATION\nz_score_std = 0\n\n#MEAN :\n#[[[21.787233, 21.787233, 21.787233]]]\n#STD :\n#[[[49.0662,   64.137405, 40.04247 ]]]","metadata":{"id":"exXEJNwnuwU1","outputId":"f66b7887-40d4-42db-8620-c6f9d3335b96"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Compute new statistics","metadata":{"id":"3L1f-wyeuG9I"}},{"cell_type":"code","source":"# TRAINING ---------------------------------------------------------------------\n\n# constructor\ntrain_data_gen = ImageDataGenerator(\n    # data augmentation\n    rotation_range=40,\n    height_shift_range=50,\n    width_shift_range=50,\n    zoom_range=0.3,\n    shear_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True,\n    brightness_range=[0.5,1.5],\n    fill_mode='reflect',\n\n    # z-score\n    featurewise_center=False, # mean center shift\n    featurewise_std_normalization=False # std normalization\n)\n\n# generator\ntrain_gen = train_data_gen.flow_from_directory(\n    directory= root_path + 'training',\n    target_size=(256,256),\n    color_mode='rgb',\n    classes=None,\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=seed\n)\n\n# callable generator method\ndef CallableGenerator():\n  # yielding batch's samples at run time when called from the generator\n  for (x,y) in train_gen:\n    yield (x,y)\n\n# TENSOR-BATCHED DATASET (from generator)\nbatched_dataset = tf.data.Dataset.from_generator(\n    # above method for run time\n    CallableGenerator,\n\n    # tensor fields types\n    output_types=(tf.float32, tf.float32)\n) \n\n# TENSOR-UNBATCHED DATASET (from batched-dataset)\nmean = tf.Variable(\n    # RGB dimension        \n    np.zeros((3), dtype=np.float32)\n)\nmean_2 = tf.Variable(\n    # RGB dimension        \n    np.zeros((3), dtype=np.float32)\n)\nstd = tf.Variable(\n    # RGB dimension        \n    np.zeros((3), dtype=np.float32)\n)\n\n# visualization variables\ni = 0 # counter\ntot_epoch = math.ceil(train_gen.samples / 32) # total epoch\n\n# ONE EPOCH LOADING\nfor (x, _) in iter(batched_dataset):\n  # update visualization\n  clear_output(wait=True)\n  i += 1\n  print(\"Epoch (1/1) - Batch : \" + str(i) + \"/\" + str(tot_epoch))\n\n  # incremental mean\n  mean.assign_add(\n      tf.math.divide(\n        tf.math.reduce_sum(x, axis=[0, 1, 2]),\n        256 * 256 * train_gen.samples\n      )\n  )\n\n  # incremental mean squared\n  mean_2.assign_add(\n      tf.math.divide(\n        tf.math.reduce_sum(tf.math.square(x), axis=[0, 1, 2]),\n        256 * 256 * train_gen.samples\n      )\n  )\n    \n  # end condition, otherwise unlimited\n  if i > tot_epoch:\n      break\n\n# standard deviation\nstd = tf.math.sqrt(\n    tf.math.subtract(mean_2, mean)\n)\n\n# reshape\nmean = tf.reshape(mean, (1, 1, 3))\nstd = tf.reshape(std, (1, 1, 3))\n\n# visualization\nprint('training set mean :', mean)\nprint('training set std :', std)","metadata":{"id":"_G0kvvl2JrWG","outputId":"580ced95-87f0-4098-f3e8-fe93953a9e15","execution":{"iopub.status.busy":"2021-11-17T20:46:45.467072Z","iopub.execute_input":"2021-11-17T20:46:45.467505Z","iopub.status.idle":"2021-11-17T20:50:24.980807Z","shell.execute_reply.started":"2021-11-17T20:46:45.467472Z","shell.execute_reply":"2021-11-17T20:50:24.979732Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Z-SCORE activation\ntrain_data_gen.featurewise_center = True\ntrain_data_gen.featurewise_std_normalization = True\n\n# statistics fitting computation on unbatched_dataset\ntrain_data_gen.fit(unbatched_dataset)\n\n# new statistics just computed\nz_score_mean = train_data_gen.mean\nz_score_std = train_data_gen.std\n\n# visualization\nprint('MEAN :')\nprint(z_score_mean)\nprint('STD :')\nprint(z_score_std)","metadata":{"id":"nJTSXj9ymosb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generators","metadata":{"id":"-DDOJSRiu2fU"}},{"cell_type":"markdown","source":"#### Training set","metadata":{"id":"lonSvykmshfS"}},{"cell_type":"markdown","source":"##### Pre-Computed statistics","metadata":{"id":"xzUafTrvvSEd"}},{"cell_type":"code","source":"# TRAINING ---------------------------------------------------------------------\n\n# constructor\ntrain_data_gen = ImageDataGenerator(\n    # z-score\n    featurewise_center=True, # mean center shift\n    featurewise_std_normalization=True # std normalization\n)\n\n# training set statistics\ntrain_data_gen.mean = z_score_mean\ntrain_data_gen.std = z_score_std\n\n# generator\ntrain_gen = train_data_gen.flow_from_directory(\n    directory=training_dir,\n    target_size=(256,256),\n    color_mode='rgb',\n    classes=None,\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=seed\n)","metadata":{"id":"HJSVlAqbsnsh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Use new statistics","metadata":{"id":"yOxEcJKbvaoG"}},{"cell_type":"code","source":"# generator\ntrain_gen = train_data_gen.flow_from_directory(\n    directory=training_dir,\n    target_size=(256,256),\n    color_mode='rgb',\n    classes=None,\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=seed\n)","metadata":{"id":"MBng2x5fvi_F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Validation set","metadata":{"id":"fek6lP2Tqv4e"}},{"cell_type":"code","source":"# VALIDATION -------------------------------------------------------------------\n\n# constructor\nvalid_data_gen = ImageDataGenerator(\n    # z-score\n    featurewise_center=True, # mean center shift\n    featurewise_std_normalization=True # std normalization\n)\n\n# training set statistics\nvalid_data_gen.mean = z_score_mean\nvalid_data_gen.std = z_score_std\n\n# generator\nvalid_gen = valid_data_gen.flow_from_directory(\n    directory=validation_dir,\n    target_size=(256,256),\n    color_mode='rgb',\n    classes=None,\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=seed\n)","metadata":{"id":"g5tB0i1crCoB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Testing set","metadata":{"id":"AtK_zPlnqrR5"}},{"cell_type":"code","source":"# TESTING ----------------------------------------------------------------------\n\n# constructor\ntest_data_gen = ImageDataGenerator(\n    # z-score\n    featurewise_center=True, # mean center shift\n    featurewise_std_normalization=True # std normalization\n)\n\n# training set statistics\ntest_data_gen.mean = z_score_mean\ntest_data_gen.std = z_score_std\n\n# generator\ntest_gen = test_data_gen.flow_from_directory(\n    directory=testing_dir,\n    target_size=(256,256),\n    color_mode='rgb',\n    classes=None,\n    class_mode='categorical',\n    batch_size=8,\n    shuffle=True,\n    seed=seed\n)","metadata":{"id":"PpK2fC4zwqNO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cross validation\ntrain_valid_data_gen = ImageDataGenerator(preprocessing_function=leaf_preprocess,\n                                          rotation_range=30,\n                                          height_shift_range=50,\n                                          width_shift_range=50,\n                                          zoom_range=0.3,\n                                          horizontal_flip=True,\n                                          vertical_flip=True,\n                                          fill_mode='reflect', #)\n                                          rescale=1/255.)\n\ntrain_valid_gen = train_data_gen.flow_from_directory(directory=training_dir,\n                                               target_size=(256,256),\n                                               #color_mode='rgb',\n                                               classes=None, # can be set to labels\n                                               class_mode='categorical',\n                                               batch_size=8,\n                                               shuffle=True,\n                                               seed=seed)","metadata":{"id":"3dw49jccc1AS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Design","metadata":{"id":"_Nuy88eiyZVA"}},{"cell_type":"code","source":"# labels\nlabels = ['Apple','Blueberry','Cherry','Corn','Grape','Orange','Peach','Pepper','Potato','Raspberry','Soybean','Squash','Strawberry','Tomato']","metadata":{"id":"72TxDCRj88ro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model hyperparameters\ninput_shape = (256, 256, 3)\nepochs = 100\nbatch_size = 64\nn_classes = len(labels)\nweight_decay = 1e-3\nmodel_name = \"CNN_bw_sharp_gap\"","metadata":{"id":"ZTORqKWWzu-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model skeleton\n# > (Conv2D + BN + ReLu + MaxPoolAvg) x 3 +\n# + (Conv2D + L2 + BN + ReLu + MaxPoolAvg) x 1 + \n# + (Conv2D + BN + ReLu + GlobalPoolAvg) x 1 +\n# + SoftMax x 1\ndef mirknet(input_shape):\n    # (0) input\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n\n    # (1) Conv + L2 + BN + ReLu + MaxPoolAvg\n    conv1 = tfkl.Conv2D(\n        filters=16,\n        kernel_size=(3, 3),\n        strides = (1, 1),\n        padding = 'same',\n        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n    )(input_layer)\n    batch_norm1 = tfkl.BatchNormalization()(conv1)\n    relu1 = tf.keras.layers.ReLU()(batch_norm1)\n    pool1 = tfkl.MaxPooling2D(\n        pool_size = (2, 2)\n    )(relu1)\n\n    # (2) Conv + L2 + BN + ReLu + MaxPoolAvg\n    conv2 = tfkl.Conv2D(\n        filters=32,\n        kernel_size=(3, 3),\n        strides = (1, 1),\n        padding = 'same',\n        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n    )(pool1)\n    batch_norm2 = tfkl.BatchNormalization()(conv2)\n    relu2 = tf.keras.layers.ReLU()(batch_norm2)\n    pool2 = tfkl.MaxPooling2D(\n        pool_size = (2, 2)\n    )(relu2)\n\n    # (3) Conv + L2 + BN + ReLu + MaxPoolAvg\n    conv3 = tfkl.Conv2D(\n        filters=64,\n        kernel_size=(3, 3),\n        strides = (1, 1),\n        padding = 'same',\n        activation = 'relu',\n        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n    )(pool2)\n    batch_norm3 = tfkl.BatchNormalization()(conv3)\n    relu3 = tf.keras.layers.ReLU()(batch_norm3)\n    pool3 = tfkl.MaxPooling2D(\n        pool_size = (2, 2)\n    )(relu3)\n\n    # (4) Conv + L2 + BN + ReLu + MaxPoolAvg\n    conv4 = tfkl.Conv2D(\n        filters=128,\n        kernel_size=(3, 3),\n        strides = (1, 1),\n        padding = 'same',\n        activation = 'relu',\n        kernel_initializer = tfk.initializers.GlorotUniform(seed),\n        kernel_regularizer=tf.keras.regularizers.l2(weight_decay)\n    )(pool3)\n    batch_norm4 = tfkl.BatchNormalization()(conv4)\n    relu4 = tf.keras.layers.ReLU()(batch_norm4)\n    pool4 = tfkl.MaxPooling2D(\n        pool_size = (2, 2)\n    )(relu4)\n\n    # (5) Conv + L2 + BN + ReLu + MaxPoolAvg\n    conv5 = tfkl.Conv2D(\n        filters=256,\n        kernel_size=(3, 3),\n        strides = (1, 1),\n        padding = 'same',\n        activation = 'relu',\n        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n    )(pool4)\n    batch_norm5 = tfkl.BatchNormalization()(conv5)\n    relu5 = tf.keras.layers.ReLU()(batch_norm5)\n    pool5 = tfkl.MaxPooling2D(\n        pool_size = (4, 4)\n    )(relu5)\n\n    # (6) Output\n    flattening_layer = tfkl.Flatten(name='Flatten')(pool5)\n\n    flattening_layer = tfkl.Dropout(0.5, seed=seed)(flattening_layer)\n    classifier_layer = tfkl.Dense(units=512,\n                                  name='Classifier_1',\n                                  kernel_initializer=tfk.initializers.GlorotUniform(seed),\n                                  activation='relu')(flattening_layer)\n\n    flattening_layer = tfkl.Dropout(0.5, seed=seed)(classifier_layer)\n    classifier_layer = tfkl.Dense(units=256,\n                                  name='Classifier_2',\n                                  kernel_initializer=tfk.initializers.GlorotUniform(seed),\n                                  activation='relu')(flattening_layer)\n\n    output_layer = tfkl.Dense(units=n_classes, activation='softmax', \n                              kernel_initializer=tfk.initializers.GlorotUniform(seed), \n                              name='Output')(classifier_layer)'''\n\n    # Model connection\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    # Model Compilation\n    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n\n    # Return the model\n    return model","metadata":{"id":"fUYcW3XBycYU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model skeleton\n# > (Conv2D + BN + ReLu + MaxPoolAvg) x 4 +\n# + (Conv2D + BN + ReLu + GlobalAvgPool) x 1 +\n# + SoftMax x 1\ndef mirknet(input_shape):\n    # (0) input\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n\n    # (1) Conv + L2 + BN + ReLu + MaxPoolAvg\n    conv1 = tfkl.Conv2D(\n        filters=16,\n        kernel_size=(3, 3),\n        strides = (1, 1),\n        padding = 'same',\n        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n    )(input_layer)\n    batch_norm1 = tfkl.BatchNormalization()(conv1)\n    relu1 = tf.keras.layers.ReLU()(batch_norm1)\n    pool1 = tfkl.MaxPooling2D(\n        pool_size = (2, 2)\n    )(relu1)\n\n    # (2) Conv + L2 + BN + ReLu + MaxPoolAvg\n    conv2 = tfkl.Conv2D(\n        filters=32,\n        kernel_size=(3, 3),\n        strides = (1, 1),\n        padding = 'same',\n        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n    )(pool1)\n    batch_norm2 = tfkl.BatchNormalization()(conv2)\n    relu2 = tf.keras.layers.ReLU()(batch_norm2)\n    pool2 = tfkl.MaxPooling2D(\n        pool_size = (2, 2)\n    )(relu2)\n\n    # (3) Conv + L2 + BN + ReLu + MaxPoolAvg\n    conv3 = tfkl.Conv2D(\n        filters=64,\n        kernel_size=(3, 3),\n        strides = (1, 1),\n        padding = 'same',\n        activation = 'relu',\n        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n    )(pool2)\n    batch_norm3 = tfkl.BatchNormalization()(conv3)\n    relu3 = tf.keras.layers.ReLU()(batch_norm3)\n    pool3 = tfkl.MaxPooling2D(\n        pool_size = (2, 2)\n    )(relu3)\n\n    # (4) Conv + L2 + BN + ReLu + MaxPoolAvg\n    conv4 = tfkl.Conv2D(\n        filters=128,\n        kernel_size=(3, 3),\n        strides = (1, 1),\n        padding = 'same',\n        activation = 'relu',\n        kernel_initializer = tfk.initializers.GlorotUniform(seed),\n        kernel_regularizer=tf.keras.regularizers.l2(weight_decay)\n    )(pool3)\n    batch_norm4 = tfkl.BatchNormalization()(conv4)\n    relu4 = tf.keras.layers.ReLU()(batch_norm4)\n    pool4 = tfkl.MaxPooling2D(\n        pool_size = (2, 2)\n    )(relu4)\n\n    # (5) Conv + L2 + BN + ReLu + MaxPoolAvg\n    conv5 = tfkl.Conv2D(\n        filters=256,\n        kernel_size=(3, 3),\n        strides = (1, 1),\n        padding = 'same',\n        activation = 'relu',\n        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n    )(pool4)\n    batch_norm5 = tfkl.BatchNormalization()(conv5)\n    relu5 = tf.keras.layers.ReLU()(batch_norm5)\n    pool5 = tfkl.MaxPooling2D(\n        pool_size = (2, 2)\n    )(relu5)\n    gap = tfkl.GlobalAveragePooling2D()(pool5)\n\n    # (6) Output\n\n    output_layer = tfkl.Dense(units=n_classes, activation='softmax', \n                              kernel_initializer=tfk.initializers.GlorotUniform(seed), \n                              name='Output')(gap)\n\n    # Model connection\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    # Model Compilation\n    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n\n    # Return the model\n    return model","metadata":{"id":"t2GKgTsVEXuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build model\nmodel = mirknet(input_shape)\nmodel.summary()","metadata":{"id":"wWxEMZ4Tzn5F","outputId":"d59b3c22-9982-4b21-8632-a09552e0050f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility function to create folders and callbacks for training\nfrom datetime import datetime\n\ndef create_folders_and_callbacks(model_name):\n\n  exps_dir = os.path.join(root_path+'/new_model_gap')\n  if not os.path.exists(exps_dir):\n      os.makedirs(exps_dir)\n\n  now = datetime.now().strftime('%b%d_%H-%M-%S')\n\n  exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n  if not os.path.exists(exp_dir):\n      os.makedirs(exp_dir)\n      \n  callbacks = []\n\n  # Model checkpoint\n  # ----------------\n  ckpt_dir = os.path.join(exp_dir, 'ckpts')\n  if not os.path.exists(ckpt_dir):\n      os.makedirs(ckpt_dir)\n\n  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'), \n                                                     save_weights_only=False, # True to save only weights\n                                                     save_best_only=False) # True to save only the best epoch \n  callbacks.append(ckpt_callback)\n\n  # Early Stopping\n  # --------------\n  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n  callbacks.append(es_callback)\n\n  return callbacks","metadata":{"id":"9hBriTOK51E7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create folders and callbacks and fit\ncallbacks = create_folders_and_callbacks(model_name='CNN_full')\n\n# Train the model\nhistory = model.fit(\n    x = train_gen,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_data = valid_gen,\n    callbacks=[callbacks]\n).history","metadata":{"id":"QR-oE0mC6vrh","outputId":"36e8e713-47b6-45b8-a1fc-7a9098fbefd9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save best epoch model\nmodel.save(\"models/CNN_full_best_\" + str(datetime.now().strftime('%b%d_%H-%M-%S')))","metadata":{"id":"HK13X-hm6zxR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-Validation","metadata":{"id":"QC_p087NHcah"}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nnum_folds = 10\n\nhistories = []\nscores = []\n\nkfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n\nfor fold_idx, (train_idx, valid_idx) in enumerate(kfold.split(X_train_val, y_train_val)):\n\n  print(\"Starting training on fold num: {}\".format(fold_idx+1))\n\n  model = mirknet(input_shape)\n\n  history = model.fit(\n    x = X_train_val.iloc[train_idx],\n    y = y_train_val.iloc[train_idx],\n    validation_data=(X_train_val.iloc[valid_idx], y_train_val.iloc[valid_idx]),\n    batch_size = batch_size,\n    epochs = 100,\n    callbacks=[early_stopping]\n  ).history\n\n  score = model.evaluate(X_train_val.iloc[valid_idx], y_train_val.iloc[valid_idx])\n  scores.append(score[1])\n\n  histories.append(history)","metadata":{"id":"8HgwitvSHg9L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{"id":"INqqmJjZHhZn"}},{"cell_type":"code","source":"mean_accuracy = 0\nn = 0\n\ny_pred = np.array([])\ny_test = np.array([])\n\nfor i, (sample, target) in enumerate(test_gen):\n  y_pred = np.append(y_pred, np.argmax(model.predict(sample)))\n  mean_accuracy += 1 if np.argmax(model.predict(sample)) == np.argmax(target) else 0\n  n = i\nmean_accuracy /= n\n\nprint(\"mean accuracy (test set) : \" + str(mean_accuracy))","metadata":{"id":"RnobBS5c_J7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prova = np.array([])\nprova2 = np.array([1,2,3,4,5,6,7])\n\nfor num in prova2:\n  prova = np.append(prova, num)\n  print(prova)\n\nprova.item(2)","metadata":{"id":"r_m5Yi6ZCByQ","outputId":"17737d1e-b7b8-4697-ccfe-491164aa69bf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2])\ny_test = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2])\nlabels = [\"Cats\", \"Dogs\", \"Horses\"]\n\ncm = confusion_matrix(y_test, y_pred)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","metadata":{"id":"mBz-jsrRBj2n"},"execution_count":null,"outputs":[]}]}